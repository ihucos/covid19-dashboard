#!/usr/bin/env python3
import urllib.request
import json
import re

URL = 'https://www.tagesschau.de/allemeldungen/'
REGEX = '<a href="(/newsticker/liveblog-?coro.*?)".*?>(.*?)</a>'
DETAIL_PAGE_LIST_REGEX = '<div class="checklist small"><ul>(.*?)</ul></div>'

with urllib.request.urlopen(URL) as response:
    html = response.read().decode()
matches = re.findall(REGEX, html)
assert len(
    matches
) == 7, "did not found one entry for the last 7 days, found {}".format(
    len(matches))
summary = []
for url_part, link_text in matches:
    link_text = link_text.split(": ")[-1].strip(" +")
    summary.append({
        "link":
        "https://www.tagesschau.de{}".format(url_part),
        "text":
        link_text
    })

with urllib.request.urlopen(URL) as response:
    html = response.read().decode()

#
# parse single page entry
#

last_days = []
for entry in summary:
    this_day = []
    link_today = entry["link"]
    with urllib.request.urlopen(link_today) as response:
        html = response.read().decode()
    match, = re.findall(DETAIL_PAGE_LIST_REGEX, html)
    for headline in re.findall("<li>(.*?)</li>", match):
        this_day.append({"link": link_today, "text": headline})
    last_days.append(this_day)

data = {"summary": summary, "last_days": last_days}
with open("build/tagesschau.json", "w") as f:
    f.write(json.dumps(data, indent=2))
